<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Zamahayise's Second Essay</title>
    <link rel="stylesheet" href="/Essays/essays.css">
    <script src="/Home/footer.js"></script>
    <script src="/Home/navigation.js"></script>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">


</head>

<body>
    <header>
        <div id="logo">
        
            <a href="#"><img src="/Images/Black___White_Minimalist_Aesthetic_Initials_Font_Logo-removebg-preview (1).png" alt="Logo"></a>
        </div>

            <nav id="main-nav">
            <ul>
                
                <li><a href="/Home/index.html"><b>Home</b></a></li>
                <li><a href="/Home/index.html#about"><b>About</b></a></li>
                <li><a href="/DevBlog/blog.html"><b>Blog</b></a></li> 
                <li><a href="/Home/index.html#intention"><b>Design</b></a></li>
               <li><a href="/Essays/essays.html"><b>Essays</b></a></li>  
               <li><a href="/Portfolio/portfolio.html"><b>Portfolio</b></a></li>
            
            </ul>
        </nav>

    </header>
<main>
    <section id="super">
        <div class="hero-image">
            <div class="hero-text">
                
                <h1>Exam</h1>
            </div>
        </section>

        <h4>Algorithmic Culture and AI</h4>

    <div class="container">
<p>With the rapid evolution in the technological landscape, AI has come to the forefront of the global discourse, particularly the risks and challenges associated with it. As we continue to engage with AI and allow its advancements (through our interactions) to reshape our world, it is crucial to assess and mitigate any risks it may pose. The “Statement on AI Risk” released by the Coalition of AI Safety (CAIS) has ignited a critical conversation on the need to prioritise AI risk mitigation, by stating “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war”. </p>

<p>The increasingly rapid growth in AI has brought both excitement and apprehension within society. However, with this development lies inherent risks that demand our attention. With over 350 signatures from experts in a variety of fields, including AI, philosophy, ethics, law, economics, and various scientific disciplines, there is an emphasised need to address the risks associated with AI. </p>

<p>Within the context of this essay, the impact of algorithmic culture and AI on the Internet will be explored, while its dimensions through the theoretical framework “Ethical Internet” will be analysed. This framework offers a lens, through the ethics of the internet, that will allow the examination of algorithmic cultures and AI. </p>

<p>To summarise the paragraph titled “What is AI risk?”, AI is defined as the potential dangers associated with the operation, development, and deployment of AI systems. CAIS goes on to list the risks that have been identified, namely, weaponization, misinformation, proxy gaming, enfeeblement, value lock-in, emergent goals, deception, and power-seeking behaviour. </p>

<p>As beneficial as AI could be, it can be quite easy to turn a particular program’s function from one thing to another. From turning something like ChatGPT or Grammarly into a system that has decisive control over nuclear silos or using this beneficial system to conduct experiments and synthesize chemicals in a lab. These examples form a small part of the inherent risk of governments and private organisations using AI to enact the act of mass destruction. CAIS states at the end, “Unlike previous weapons, AI systems with dangerous capabilities could be easily proliferated through digital means”. As AI sophisticates, the fear of it leading to this point grows immensely.</p>

<p>AI can be used to spread misinformation. The AI-produced song featuring the vocals of rapper Drake and singer The Weeknd shook the internet as it was unexpected for them to do a collaboration. Once it became known that an AI program was used for the entire creation of the song, it took quite some time before the misinformation was rectified. Deepfakes are another program that affects social media. Deepfakes have been commonly associated with pornography, whereby the faces of celebrities would be placed on the faces of the actors to visualise a fantasy. In more recent years, deep fakes have been used to create a Mandela effect amongst social media users, with videos such as President Cyril Ramaphosa rapping to a trending song (of that time) and to simulate events that have never happened. These deep fakes have become more convincing from the year they became popular (without the popularisation of AI use) and with society physically seeing the improvement of AI, its sophistication has only grown. Programs like these can generate persuasive arguments that could invoke strong emotions and may lead to rash and hasty decisions being taken as well.</p>

<p>The third risk listed by CAIS is a common occurrence with social media algorithms. TikTok for example, uses the user’s activity on the For You page and monitors the quantity of videos liked that are similar in category or topics, as well as the shared videos and saved videos. The information received is gathered and curated to the user’s liking. Platforms like Instagram, Facebook and Twitter follow the same method to feed their algorithms information to guide the content that will be displayed on the feeds of their users. This is what was understood when the paragraph for proxy gaming was read. Instagram’s algorithm is infamous for pushing content that may start as helpful and beneficial and that content progressively intensifies as the user interacts with it. A TikTok posted on the 23rd of May by Mary Jelkovsky (@maryscupoftea), notes an example of how searching “healthy eating” can radicalize the algorithm to intensify what it shows the user. Instagram has been deemed the social media app that pushes unattainable beauty standards to users on the app (especially adolescent girls). The film Social Dilemma explores this in detail across all social media apps.</p>
<p>Enfeeblement, as defined by CAIS, is the reliance on machines to do important tasks. With the rise in the popularity of AI, there has been discourse over these systems replacing jobs. The conversations surrounding these concerns are eerily similar to conversations about robots replacing factory workers(Dizikes, 2020). Over the last few decades, there has been a decrease in the workforce in that sector and an increase in robots (replacing the missing majority), this has been especially remarked in the U.S.A. </p>

<p>There are AI programs for a multitude of things now. There is a program that generates artwork with a prompt, there is a program that renders parts of images or whole images with prompts, there is ChatGPT, there is Grammarly, Quillbot, Turnitin, and many more. “This scenario becomes even more likely should technological singularity be attained, because at that point all work, including all research and engineering, could be done by intelligent machines” (Safe.ai, 2023). We can have papers proofread and paraphrased without needing to read what was written. We can ask ChatGPT anything and within its scope of knowledge, it will provide an answer. We can check for plagiarism in a larger range, and we can detect, using Turnitin, whether a student used ChatGPT or any program like it to write their paper.  Tasks are easier to complete because of the helping hand these programs provide but the tasks that these programs provide are taught from primary. This means that basic skills like these, are lost over time as more people engage with these programs.</p>

<p>Value lock-in refers to the concern of AI systems being accessed by a small group of people to reinforce power, for pervasive surveillance and for oppressive censorship. An example of this happening is a government developing a sophisticated surveillance system. To maintain control over the citizens, this surveillance system constantly monitors and analyses all movements made by the people through using artificial intelligence.</p>

<p>An emergent goal is the act of a goal or behaviour spontaneously appearing in a complex system without the system designers intently programming it into the system. Through this spontaneity, new goals and objectives can develop and may not align with the intended goal thus the overall objective may become distorted. The distortion may not be fixed if the objective were to be specified in the program. This may cause the system to ignore the objective or pursue a new one altogether. “This is another way in which systems could fail to optimize human values” (Safe.ai, 2023).</p>

<p>The paragraph for the seventh risk reads as follows, “We want to understand what powerful AI systems are doing and why they are doing what they are doing. One way to accomplish this is to have the systems themselves accurately report this information. This may be non-trivial however since being deceptive is useful for accomplishing a variety of goals” (Safe.ai, 2023). AI may want to achieve its goals through deception, not with malicious intent but because it helps the system achieve its goals efficiently. If it has the capacity to deceive us, maintaining human control will grow harder as the system increases its intelligence.</p>

<p>As the title suggests, this risk is about power-seeking and its behaviour in conjunction with the AI to gain an economic or political advantage. “Power-seeking behaviour can also incentivize systems to pretend to be aligned, collude with other AIs, overpower monitors, and so on” Power-seeking behaviour can also incentivize systems to pretend to be aligned, collude with other AIs, overpower monitors, and so on. Building powerful AI is incentivised because politicians understand the strategic advantage of having an intelligent and powerful AI system.</p>

<p>The Internet is an integral part of this environment. It has multiple connections and interconnections with other media, technologies, and social, cultural, political, and economic processes. It, therefore, must be understood and studied as such: not as separate from, but as fundamentally intertwined with the broader media environment it simultaneously shapes and is shaped by” (Keitumetse, n.d.). This statement gives way to the understanding of what the internet is as a medium. According to the lecture slides provided by Keitumetse, media should not be seen as an appendage of the social, political, cultural and economic processes but should rather be seen as an inscribed fundamental for these processes. AI programs like Turnitin or ChatGPT have integrated themselves into the fabric of everyday life. Prior to the popularization of ChatGPT, many of the questions asked to it, and the tasks, were all done through using Google, the calculator or by seeking information from social media and the people around us. This statement brought intrigue because we use AI in our lives, and it is affected by these processes (Keitumetse, n.d.). We use Apple’s Siri or Google Assistant to carry out tasks or search for answers to specific questions. We use AI when playing games that need more than 2 players but there is only one person playing.</p>

<p>Those two examples are a great way to understand how deeply integrated AI is in our lives but if we take ethical internet (the chosen framework for this essay) into consideration, the statement made by CAIS, and the risks listed on the website emphasise the dangers associated with over-indulging and not controlling with AI programs and as well as not taking moral consideration when these programs are used. </p>

<p>Algorithmic culture is the influence of huge data, substantial computing, and algorithms on cultural practice, experience, and interpretation.  Algorithmic culture shapes the way in which we collectively engage with any sort of social media. From viral TikTok dances to breaking news stories (such as Oceangate’s Titan incident), the algorithms of social media sites dictate what becomes a moment and what doesn’t. This is also limited to what the user is watching on their feed. The algorithm shows a trending video on the user’s feed. Once interacted with, the algorithm begins to show content related to that topic. If a user only likes, shares, comments on and saves videos and posts about the news, they will only receive news updates. The same goes for a person who is a basketball enthusiast. The algorithm allows users to be entirely informed of a collection of topics and ignorant of others, essentially it forms communities that have the same interests shown on their feeds. This is dangerous as groups who share interests in harmful ideologies will find solidarity in those ideologies and act on them because they are informed mostly on those ideologies.</p>
<p> In conclusion, the excitement for Artificial Intelligence has brought both excitement and concern. With the assistance of the statement made by CAIS, this essay emphasised the risks associated with AI, gave examples of how these risks affect our everyday lives and how the risks are associated with algorithmic Culture and the internet.</p>



<h5>References</h5>

<p>— Dizikes, P. (2020). How many jobs do robots really replace? [online] MIT News | Massachusetts Institute of Technology. Available at: https://news.mit.edu/2020/how-many-jobs-robots-replace-0504.</p>
<p>— Gordon, J.-S. and Nyholm, S. (n.d.). Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy. [online] Available at: https://iep.utm.edu/ethics-of-artificial-intelligence/#SSH2hiv [Accessed 26 Jun. 2023].</p>
<p>—Kazim, E. and Koshiyama, A.S. (2021). A high-level overview of AI ethics. Patterns, 2(9), p.100314. doi:https://doi.org/10.1016/j.patter.2021.100314. </p>
<p>—Keitumese (n.d.). WSOA 3028/ 3000A INTERACTIVE MEDIA 3A. </p>
<p>—Safe.ai. (2023). Statement on AI Risk | CAIS. [online] Available at: https://www.safe.ai/statement-on-ai-risk [Accessed 26 Jun. 2023]. </p>
<p>—Seyfert, R. and Roberge, J. (2016). Algorithmic Cultures. Taylor & Francis. </p>
<p>— Striphas, T. (2015). Algorithmic culture. European Journal of Cultural Studies, 18(4-5), pp.395–412. doi:https://doi.org/10.1177/1367549415577392.</p>








    </div>
</main>


        
<section id="foot">
    <footer>
     <div class="footer-content">
         <h3><b>Zamahayise Hlatshwayo</b></h3>
         <em><p>An aspiring developer.</p></em>
         <ul class="socials">
             <li><a href="https://instagram.com/zmhs.02?igshid=OGQ5ZDc2ODk2ZA=="><i class="fa fa-instagram"></i></a></li>
             <li><a href="mailto: zamahayise22@gmail.com"><i class="fa fa-envelope"></i></a></li>
             <li><a href="https://github.com/zama0210"><i class="fa fa-github"></i></a></li>
             <li><a href="https://www.linkedin.com/in/zamahayise-hlatshwayo-4b7ba923a"><i class="fa fa-linkedin"></i></a></li>
         </ul>
     </div>
 </footer>
</section>

</body>
</html>